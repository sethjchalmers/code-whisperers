# Example environment configuration
# Copy to .env and fill in your values

# =============================================================================
# LLM Provider Configuration
# =============================================================================

# Provider: github-models, ollama, openai, azure, copilot
LLM_PROVIDER=github-models

# Model name (depends on provider)
# github-models: gpt-4o, gpt-4o-mini, meta-llama-3.1-70b-instruct, mistral-large-2411
# ollama: llama3.1, codellama, mistral, deepseek-coder
# openai: gpt-4-turbo, gpt-4, gpt-3.5-turbo
LLM_MODEL=gpt-4o

# Temperature (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.1

# =============================================================================
# API Keys (only set the ones you need)
# =============================================================================

# GitHub Token - for GitHub Models API (free tier)
# Get from: https://github.com/settings/tokens
# No special scopes required
GITHUB_TOKEN=

# OpenAI API Key - for OpenAI provider
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Azure OpenAI - for Azure provider
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=

# =============================================================================
# Local/Proxy Endpoints
# =============================================================================

# Ollama endpoint (if using Ollama provider)
OLLAMA_ENDPOINT=http://localhost:11434

# Copilot proxy endpoint (if using Copilot provider)
COPILOT_ENDPOINT=http://localhost:11435

# =============================================================================
# Git Configuration
# =============================================================================

# Default base branch for comparisons
GIT_BASE_BRANCH=main

# =============================================================================
# Review Settings
# =============================================================================

# Maximum file size to review (in KB)
MAX_FILE_SIZE_KB=500

# Run agents in parallel (faster but uses more API calls)
PARALLEL_AGENTS=true

# Include repository context files (.gitignore, README, etc.)
INCLUDE_REPO_CONTEXT=true
